# =============================================================================
# SpeakInsights v3 — Docker Compose (Mac ARM64 / Default Profile)
# =============================================================================
# Usage:
#   Mac:     docker compose up --build -d
#   Windows: docker compose -f docker-compose.yml -f docker-compose.windows.yml up --build -d
#
# On Mac, Ollama runs NATIVELY for Metal GPU acceleration.
# Backend connects to Ollama via host.docker.internal:11434.
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Frontend — React + Vite, served by Nginx
  # ---------------------------------------------------------------------------
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: speakinsights-frontend
    ports:
      - "3000:80"
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - speakinsights-net
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 256M
          cpus: "0.5"

  # ---------------------------------------------------------------------------
  # Backend — FastAPI + SQLAlchemy (async)
  # ---------------------------------------------------------------------------
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: speakinsights-backend
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      livekit:
        condition: service_started
    environment:
      # Database
      - DATABASE_URL=postgresql+asyncpg://postgres:${POSTGRES_PASSWORD:-speakinsights}@postgres:5432/speakinsights
      # Redis
      - REDIS_URL=redis://redis:6379/0
      # LiveKit
      - LIVEKIT_URL=ws://livekit:7880
      - LIVEKIT_API_KEY=${LIVEKIT_API_KEY:-devkey}
      - LIVEKIT_API_SECRET=${LIVEKIT_API_SECRET:-devsecret1234567890}
      # WhisperX
      - WHISPERX_URL=http://whisperx:9000
      # Ollama — on Mac, connects to host machine (native Ollama)
      - OLLAMA_URL=${OLLAMA_URL:-http://host.docker.internal:11434}
      - OLLAMA_MODEL=${OLLAMA_MODEL:-llama3.2:3b}
      # Storage
      - STORAGE_PATH=/app/storage
      # CORS
      - CORS_ORIGINS=${CORS_ORIGINS:-http://localhost:3000,http://127.0.0.1:3000}
      # HuggingFace token (for WhisperX diarization if needed)
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - ./storage:/app/storage
    networks:
      - speakinsights-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 20s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2.0"

  # ---------------------------------------------------------------------------
  # PostgreSQL 16 + pgvector — Vector-enabled database
  # ---------------------------------------------------------------------------
  postgres:
    image: pgvector/pgvector:pg16
    container_name: speakinsights-postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-speakinsights}
      - POSTGRES_DB=speakinsights
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./scripts/init-db.sql:/docker-entrypoint-initdb.d/init-db.sql:ro
    networks:
      - speakinsights-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d speakinsights"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1.0"

  # ---------------------------------------------------------------------------
  # Redis 7 — Cache & message queue
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: speakinsights-redis
    ports:
      - "6379:6379"
    command: redis-server --maxmemory 128mb --maxmemory-policy allkeys-lru
    volumes:
      - redis-data:/data
    networks:
      - speakinsights-net
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s
    deploy:
      resources:
        limits:
          memory: 192M
          cpus: "0.5"

  # ---------------------------------------------------------------------------
  # LiveKit — WebRTC SFU server
  # ---------------------------------------------------------------------------
  livekit:
    image: livekit/livekit-server:v1.7
    container_name: speakinsights-livekit
    ports:
      - "7880:7880"     # HTTP / WebSocket
      - "7881:7881"     # RTC over TCP
      - "7882:7882/udp" # RTC over UDP
    volumes:
      - ./livekit.yaml:/livekit.yaml:ro
    command: ["--config", "/livekit.yaml"]
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - speakinsights-net
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 512M
          cpus: "1.0"

  # ---------------------------------------------------------------------------
  # LiveKit Egress — Recording service
  # ---------------------------------------------------------------------------
  livekit-egress:
    image: livekit/egress:latest
    container_name: speakinsights-egress
    environment:
      - EGRESS_CONFIG_FILE=/etc/egress.yaml
    volumes:
      - ./egress.yaml:/etc/egress.yaml:ro
      - ./storage/recordings:/recordings
    depends_on:
      livekit:
        condition: service_started
      redis:
        condition: service_healthy
    networks:
      - speakinsights-net
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "2.0"

  # ---------------------------------------------------------------------------
  # WhisperX — Speech-to-text transcription service
  # ---------------------------------------------------------------------------
  whisperx:
    build:
      context: ./whisperx-service
      dockerfile: Dockerfile
    container_name: speakinsights-whisperx
    ports:
      - "9000:9000"
    environment:
      # Model configuration
      - MODEL_SIZE=${WHISPERX_MODEL:-small}
      - DEVICE=cpu
      - COMPUTE_TYPE=int8
      - BATCH_SIZE=4
      - DEFAULT_LANGUAGE=${DEFAULT_LANGUAGE:-auto}
      - HF_TOKEN=${HF_TOKEN:-}
    volumes:
      - whisperx-models:/root/.cache/huggingface
      - ./storage:/app/storage
    networks:
      - speakinsights-net
    restart: unless-stopped
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "2.0"

# =============================================================================
# Networks
# =============================================================================
networks:
  speakinsights-net:
    driver: bridge
    name: speakinsights-net

# =============================================================================
# Volumes
# =============================================================================
volumes:
  # PostgreSQL persistent data
  postgres-data:
    name: speakinsights-postgres-data

  # Redis persistent data
  redis-data:
    name: speakinsights-redis-data

  # WhisperX model cache (avoid re-downloading)
  whisperx-models:
    name: speakinsights-whisperx-models
