# =============================================================================
# SpeakInsights v3 — Docker Compose OVERRIDE for Windows with NVIDIA GPU
# =============================================================================
# Usage:
#   docker compose -f docker-compose.yml -f docker-compose.windows.yml up --build -d
#
# This file:
#   - Adds Ollama as a Docker service (with NVIDIA GPU pass-through)
#   - Adds ollama-init to pull required models on first start
#   - Overrides WhisperX to use CUDA (GPU acceleration)
#   - Points backend to Ollama container instead of host.docker.internal
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Ollama — LLM inference server (NVIDIA GPU)
  # ---------------------------------------------------------------------------
  ollama:
    image: ollama/ollama:latest
    container_name: speakinsights-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    networks:
      - speakinsights-net
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 8G

  # ---------------------------------------------------------------------------
  # Ollama Init — Pull required models on first run
  # ---------------------------------------------------------------------------
  ollama-init:
    image: curlimages/curl:latest
    container_name: speakinsights-ollama-init
    depends_on:
      ollama:
        condition: service_started
    networks:
      - speakinsights-net
    # Wait for Ollama to be ready, then pull models
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        echo "Waiting for Ollama to be ready..."
        until curl -sf http://ollama:11434/api/tags > /dev/null 2>&1; do
          echo "  Ollama not ready yet, retrying in 5s..."
          sleep 5
        done
        echo "Ollama is ready! Pulling models..."

        echo "Pulling llama3.2:3b..."
        curl -sf -X POST http://ollama:11434/api/pull \
          -H "Content-Type: application/json" \
          -d '{"name": "llama3.2:3b"}' | tail -1
        echo ""

        echo "Pulling nomic-embed-text..."
        curl -sf -X POST http://ollama:11434/api/pull \
          -H "Content-Type: application/json" \
          -d '{"name": "nomic-embed-text"}' | tail -1
        echo ""

        echo "All models pulled successfully!"
    restart: "no"

  # ---------------------------------------------------------------------------
  # Backend — Override Ollama URL to point to Docker Ollama container
  # ---------------------------------------------------------------------------
  backend:
    environment:
      - OLLAMA_URL=http://ollama:11434
    depends_on:
      ollama:
        condition: service_started

  # ---------------------------------------------------------------------------
  # WhisperX — Override for NVIDIA GPU acceleration
  # ---------------------------------------------------------------------------
  whisperx:
    environment:
      - DEVICE=cuda
      - COMPUTE_TYPE=float16
      - BATCH_SIZE=16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 4G

# =============================================================================
# Additional Volumes for Windows
# =============================================================================
volumes:
  ollama-data:
    name: speakinsights-ollama-data
